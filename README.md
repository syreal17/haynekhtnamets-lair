# haynekhtnamets-lair
Experimental. What's the difference between the processing power needed to train a model versus just use it?


# The field "as is"

The AI field has trended towards a Server-Client model, with GPT-3 as the exemplar. GPT-3 has unimaginable 
computing power behind it, but may only be accessed via a guarded API. That's all very good and justified, 
but of all computing inventions, AI has the most potential in the hands of a novice (in computing). As a
computing enthusiast and professional, I have access to high-grade consumer computing hardware. My main line
of inquiry is seeing how much GPT-2 models must reduced in parameter cardinality to be usable on high-end 
consumer hardware, and from there, can useful models be derived that can be ran on low-end computing 
hardware such as pi's or Chromebooks? Access to the Internet is fine to assume in highly-developed
countries, but not assumable for countries with more primitive infrastructure.


# Literature review

Not started.
